{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ed8269f-ba32-4ad6-9dad-d1e6037bc9fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LsiModel, LdaModel\n",
    "from gensim import corpora\n",
    "import spacy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')\n",
    "sp = spacy.load('en_core_web_lg')\n",
    "\n",
    "categories = ['rec.autos', 'comp.graphics', 'sci.space']\n",
    "newsgroup = fetch_20newsgroups(categories=categories, shuffle=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroup.data, newsgroup.target, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66af1177-933b-459b-b873-fcd688ab8646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeNLTK(input):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #Tokenizers divide strings into lists of substrings\n",
    "    wordList = word_tokenize(input)\n",
    "    output = ' '.join([lemmatizer.lemmatize(w) for w in wordList])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40b2b78f-318d-47ea-955d-71ffd5ecd19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeSpacy(input):\n",
    "    doc = sp(input)\n",
    "    #Tokenizers divide strings into lists of substrings\n",
    "    output = ' '.join([w.lemma_ for w in doc])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3cbc387-0ded-4867-b219-8e0949ae9756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NExt(input):\n",
    "    output=\"\"\n",
    "    doc = sp(input)\n",
    "    for w in doc:\n",
    "        if w.pos_==\"NOUN\":\n",
    "            output+=w.text+\" \"\n",
    "    output = output[:-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb0d038d-2961-4ec3-971a-177c44ed97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAExt(input):\n",
    "    output=\"\"\n",
    "    doc = sp(input)\n",
    "    for w in doc:\n",
    "        if w.pos_ in [\"NOUN\",\"ADJ\"]:\n",
    "            output+=w,text+\" \"\n",
    "    output = output[:-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d72d1334-3341-40d5-b3f4-908a2340216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAVExt(input):\n",
    "    output=\"\"\n",
    "    doc = sp(input)\n",
    "    for w in doc:\n",
    "        if w.pos_ in [\"NOUN\",\"ADJ\",\"VERB\"]:\n",
    "            output+=w.text+\" \"\n",
    "    output = output[:-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39dde8dd-808f-4edf-8923-7a4232b8a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NVExt(input):\n",
    "    output=\"\"\n",
    "    doc = sp(input)\n",
    "    for w in doc:\n",
    "        if w.pos_ in [\"NOUN\",\"VERB\"]:\n",
    "            output+=w.text+\" \"\n",
    "    output = output[:-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d463eaa7-3ec5-427a-ad79-c333318fb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, x_test, y_train, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    return f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2f1929f-1c48-479c-a4b5-3f1bc81fb252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textProcessing(unprocessedTexts, function=None):\n",
    "    if function is None:\n",
    "        return unprocessedTexts\n",
    "    return [function(text) for text in unprocessedTexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec5427-0587-4666-a6fd-a3bda5a9f2ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None BoW RandomForest: 0.9507556984207832\n",
      "None None BoW GBM: 0.9489297483095581\n",
      "None None TF-IDF RandomForest: 0.9524163163077691\n",
      "None None TF-IDF GBM: 0.942089878868001\n",
      "Noun None LSI RandomForest: 0.32917755634865115\n",
      "Noun None LSI GBM: 0.33365419975349053\n",
      "Noun None LDA RandomForest: 0.32585453053917934\n",
      "Noun None LDA GBM: 0.3270353861985508\n"
     ]
    }
   ],
   "source": [
    "mapPreProc = [\"None\",\"Noun\",\"Noun+Adj\",\"Noun+Adj+Verb\",\"Noun+Verb\"]\n",
    "mapProc = [\"None\", \"LemNLTK\", \"LemSpacy\"]\n",
    "mapVec = [\"BoW\",\"TF-IDF\",\"LSI\",\"LDA\"]\n",
    "mapClf= [\"RandomForest\",\"GBM\"]\n",
    "list=[]\n",
    "for i in range(4):\n",
    "    if i==0:\n",
    "        preProc=None\n",
    "    if i==1:\n",
    "        preProc=NExt\n",
    "    if i==2:\n",
    "        preProc=NAExt\n",
    "    if i==3:\n",
    "        preProc=NAVExt\n",
    "    if i==4:\n",
    "        preProc=NVExt\n",
    "    trainTarget = y_train\n",
    "    testTarget = y_test\n",
    "    trainExt = textProcessing(X_train,preProc)\n",
    "    testExt = textProcessing(X_test,preProc)\n",
    "    for j in range(3):\n",
    "        if j==0:\n",
    "            proc=None\n",
    "        if j==1:\n",
    "            proc=lemmatizeNLTK\n",
    "        if j==2:\n",
    "            proc=lemmatizeSpacy\n",
    "        trainProc = textProcessing(trainExt,proc)\n",
    "        testProc = textProcessing(testExt,proc)\n",
    "        tokenized_documents = [simple_preprocess(text) for text in trainProc+testProc]\n",
    "        dictionary = corpora.Dictionary(tokenized_documents)\n",
    "        bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "        tokTrain = [simple_preprocess(text) for text in trainProc]\n",
    "        tokTest = [simple_preprocess(text) for text in testProc]\n",
    "        trainBoW = [dictionary.doc2bow(doc) for doc in tokTrain]\n",
    "        testBoW = [dictionary.doc2bow(doc) for doc in tokTest]\n",
    "        lsi = LsiModel(bow_corpus,id2word=dictionary, num_topics=10)\n",
    "        lda = LdaModel(bow_corpus, num_topics=25, id2word=dictionary,passes=15, minimum_probability = 0)\n",
    "        for k in range(4):\n",
    "            if k==0:\n",
    "                vect = CountVectorizer(binary=False, stop_words=stop_words)\n",
    "            if k==1:\n",
    "                vect = TfidfVectorizer(stop_words=stop_words)\n",
    "            if k==2:\n",
    "                func = lsi\n",
    "            if k==3:\n",
    "                func = lda\n",
    "            if k in [0,1]:\n",
    "                trainVec = vect.fit_transform(trainProc)\n",
    "                testVec = vect.transform(testProc)\n",
    "            else:\n",
    "                trainVecPre = func[trainBoW]\n",
    "                testVecPre = func[testBoW]\n",
    "                trainVec = []\n",
    "                for p in range(len(trainVecPre)):\n",
    "                    curr = []\n",
    "                    a = trainVecPre[p]\n",
    "                    length = len(a)\n",
    "                    curr = np.array([a[b][1] for b in range(length)])\n",
    "                    trainVec.append(curr)\n",
    "                testVec = []\n",
    "                for p in range(len(testVecPre)):\n",
    "                    curr = []\n",
    "                    a = trainVecPre[p]\n",
    "                    length = len(a)\n",
    "                    curr = np.array([a[b][1] for b in range(length)])\n",
    "                    testVec.append(curr)\n",
    "                for i in range(len(trainVec)-1,0,-1):\n",
    "                    if len(trainVec[i])==0:\n",
    "                        trainVec.pop(i)\n",
    "                        trainTarget = np.delete(trainTarget, i, 0)\n",
    "                for i in range(len(testVec)-1,0,-1):\n",
    "                    if len(testVec[i])==0:\n",
    "                        testVec.pop(i)\n",
    "                        testTarget = np.delete(testTarget, i, 0)\n",
    "            for l in range(2):\n",
    "                if l==0:\n",
    "                    clf = RandomForestClassifier()\n",
    "                if l==1:\n",
    "                    clf = GradientBoostingClassifier(n_estimators=125)\n",
    "                f1 = train(trainVec, testVec, trainTarget, testTarget,clf)\n",
    "                s = \" \".join([mapPreProc[i],mapProc[j],mapVec[k],mapClf[l]])\n",
    "                list.append([f1,s])\n",
    "                print(s+\": \"+str(f1))\n",
    "listMax = [el[0] for el in list]\n",
    "maximum = max(listMax)\n",
    "print(f\"Maximum: {maximum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23ca2a-e675-461a-81a7-de514c0a4d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
