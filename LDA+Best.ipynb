{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2322b79-e4db-4a6a-a01f-335734e5ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noun+Verb LemSpacy LDA GBM: 0.8071395816658974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df240f9-8202-4f7a-ba28-9981f1908544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel\n",
    "from gensim import corpora\n",
    "import spacy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')\n",
    "sp = spacy.load('en_core_web_lg')\n",
    "\n",
    "categories = ['rec.autos', 'comp.graphics', 'sci.space']\n",
    "newsgroup = fetch_20newsgroups(categories=categories, shuffle=True)\n",
    "data = newsgroup.data\n",
    "target = newsgroup.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809834cf-87d1-48f0-922b-4ae6e0944dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeSpacy(input):\n",
    "    doc = sp(input)\n",
    "    #Tokenizers divide strings into lists of substrings\n",
    "    output = ' '.join([w.lemma_ for w in doc])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c5fcb16-5e32-43cd-ad8d-c00cfa5a9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NVExt(input):\n",
    "    output=\"\"\n",
    "    doc = sp(input)\n",
    "    for w in doc:\n",
    "        if w.pos_ in [\"NOUN\",\"VERB\"]:\n",
    "            output+=w.text+\" \"\n",
    "    output = output[:-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7f46a7-aae6-4856-a945-a20e718815b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, x_test, y_train, y_test, n):\n",
    "    clf = GradientBoostingClassifier(n_estimators=n)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    return f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51c6697-d58e-4a16-87ed-c9685abf8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textProcessing(unprocessedTexts, function=None):\n",
    "    if function is None:\n",
    "        return unprocessedTexts\n",
    "    return [function(text) for text in unprocessedTexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c76c1bd-4e6a-41b5-8da3-68f3b7244ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataExt = textProcessing(data,NVExt)\n",
    "dataProc = textProcessing(dataExt,lemmatizeSpacy)\n",
    "tokenized_documents = [simple_preprocess(text) for text in dataProc]\n",
    "dictionary = corpora.Dictionary(tokenized_documents)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "890f5987-d068-427d-8170-303489ff4d3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m mArr:\n\u001b[0;32m     10\u001b[0m     length \u001b[38;5;241m=\u001b[39m num\n\u001b[1;32m---> 11\u001b[0m     lda \u001b[38;5;241m=\u001b[39m \u001b[43mLdaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbow_corpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     dataVecPre \u001b[38;5;241m=\u001b[39m lda[bow_corpus]\n\u001b[0;32m     13\u001b[0m     len_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataVecPre)\n",
      "File \u001b[1;32m~\\PycharmProjects\\LSINewsGroup\\.venv\\Lib\\site-packages\\gensim\\models\\ldamodel.py:521\u001b[0m, in \u001b[0;36mLdaModel.__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    519\u001b[0m use_numpy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    520\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 521\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks_as_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_numpy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    524\u001b[0m     msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    525\u001b[0m )\n",
      "File \u001b[1;32m~\\PycharmProjects\\LSINewsGroup\\.venv\\Lib\\site-packages\\gensim\\models\\ldamodel.py:991\u001b[0m, in \u001b[0;36mLdaModel.update\u001b[1;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    988\u001b[0m reallen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)  \u001b[38;5;66;03m# keep track of how many documents we've processed so far\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_every \u001b[38;5;129;01mand\u001b[39;00m ((reallen \u001b[38;5;241m==\u001b[39m lencorpus) \u001b[38;5;129;01mor\u001b[39;00m ((chunk_no \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m (eval_every \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumworkers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m--> 991\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlencorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher:\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;66;03m# add the chunk to dispatcher's job queue, so workers can munch on it\u001b[39;00m\n\u001b[0;32m    995\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROGRESS: pass \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m, dispatching documents up to #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    997\u001b[0m         pass_, chunk_no \u001b[38;5;241m*\u001b[39m chunksize \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk), lencorpus\n\u001b[0;32m    998\u001b[0m     )\n",
      "File \u001b[1;32m~\\PycharmProjects\\LSINewsGroup\\.venv\\Lib\\site-packages\\gensim\\models\\ldamodel.py:847\u001b[0m, in \u001b[0;36mLdaModel.log_perplexity\u001b[1;34m(self, chunk, total_docs)\u001b[0m\n\u001b[0;32m    845\u001b[0m corpus_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(cnt \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m chunk \u001b[38;5;28;01mfor\u001b[39;00m _, cnt \u001b[38;5;129;01min\u001b[39;00m document)\n\u001b[0;32m    846\u001b[0m subsample_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m*\u001b[39m total_docs \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m--> 847\u001b[0m perwordbound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubsample_ratio\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (subsample_ratio \u001b[38;5;241m*\u001b[39m corpus_words)\n\u001b[0;32m    848\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m per-word bound, \u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;124m perplexity estimate based on a held-out corpus of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m documents with \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m words\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    850\u001b[0m     perwordbound, np\u001b[38;5;241m.\u001b[39mexp2(\u001b[38;5;241m-\u001b[39mperwordbound), \u001b[38;5;28mlen\u001b[39m(chunk), corpus_words\n\u001b[0;32m    851\u001b[0m )\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m perwordbound\n",
      "File \u001b[1;32m~\\PycharmProjects\\LSINewsGroup\\.venv\\Lib\\site-packages\\gensim\\models\\ldamodel.py:1113\u001b[0m, in \u001b[0;36mLdaModel.bound\u001b[1;34m(self, corpus, gamma, subsample_ratio)\u001b[0m\n\u001b[0;32m   1111\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbound: at document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, d)\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1113\u001b[0m     gammad, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1115\u001b[0m     gammad \u001b[38;5;241m=\u001b[39m gamma[d]\n",
      "File \u001b[1;32m~\\PycharmProjects\\LSINewsGroup\\.venv\\Lib\\site-packages\\gensim\\models\\ldamodel.py:719\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    715\u001b[0m lastgamma \u001b[38;5;241m=\u001b[39m gammad\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# We represent phi implicitly to save memory and time.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;66;03m# Substituting the value of the optimal phi back into\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;66;03m# the update for gamma gives this update. Cf. Lee&Seung 2001.\u001b[39;00m\n\u001b[1;32m--> 719\u001b[0m gammad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m+\u001b[39m expElogthetad \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mphinorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpElogbetad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    720\u001b[0m Elogthetad \u001b[38;5;241m=\u001b[39m dirichlet_expectation(gammad)\n\u001b[0;32m    721\u001b[0m expElogthetad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(Elogthetad)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alphaArr = ['auto',0.00001,0.0001,0.001,0.01,0.1,0.2,0.4,0.5,0.7,0.9,1,1.2,1.4,1.6,1.8,2,4,6,8,10,14,16,18,20,25,30]\n",
    "betaArr = ['auto',0.00001,0.0001,0.001,0.01,0.1,0.2,0.4,0.5,0.7,0.9,1,1.2,1.4,1.6,1.8,2,4,6,8,10,14,16,18,20,25,30]\n",
    "numArr = [3,5,7,10,15,20,25,30,35,40,50,60,70,80,90,100,120,140,160,180,200,225,250,275,300,350,400]\n",
    "mArr = [10,20,30,40,50,60,70,80,90,100,120,140,160,180,200,225,250,275,300,350,400,450,500]\n",
    "list = []\n",
    "for alp in alphaArr:\n",
    "    for beta in betaArr:\n",
    "        for num in numArr:\n",
    "            for m in mArr:\n",
    "                length = num\n",
    "                lda = LdaModel(bow_corpus, num_topics=num, id2word=dictionary, alpha=alp, eta=beta)\n",
    "                dataVecPre = lda[bow_corpus]\n",
    "                len_data = len(dataVecPre)\n",
    "                dataVec = [0 for o in range(len_data)]\n",
    "                for o in range(len_data):\n",
    "                    curr = []\n",
    "                    a = dataVecPre[o]\n",
    "                    lenCurr = len(a)\n",
    "                    if length!=lenCurr:\n",
    "                        curr = [0 for u in range(length)]\n",
    "                        c = 0\n",
    "                        for u in range(length):\n",
    "                            if c>=lenCurr:\n",
    "                                curr[u] = 0\n",
    "                            else:\n",
    "                                if a[c][0]!=u:\n",
    "                                    curr[u] = 0\n",
    "                                else:\n",
    "                                    curr[u] = a[c][1]\n",
    "                                    c+=1\n",
    "                        dataVec[o] = curr\n",
    "                    else:\n",
    "                        curr = [element[1] for element in a]\n",
    "                        dataVec[o] = curr\n",
    "                dataVec = np.array(dataVec)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(dataVec, target, test_size=0.33)\n",
    "                f1 = train(X_train, X_test, y_train, y_test, m)\n",
    "                list.append([f1,alp,beta,num,m])\n",
    "maxList = [w[0] for w in list]\n",
    "maximum = max(maxList)\n",
    "print(maximum)\n",
    "print(list[maxList.index(maximum)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db3acc-4d74-4c93-a65b-6fc67fd5931b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
