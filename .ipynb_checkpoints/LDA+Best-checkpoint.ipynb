{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2322b79-e4db-4a6a-a01f-335734e5ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noun+Verb LemSpacy LDA GBM: 0.8071395816658974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df240f9-8202-4f7a-ba28-9981f1908544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel\n",
    "from gensim import corpora\n",
    "import spacy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')\n",
    "sp = spacy.load('en_core_web_lg')\n",
    "\n",
    "categories = ['rec.autos', 'comp.graphics', 'sci.space']\n",
    "newsgroup = fetch_20newsgroups(categories=categories, shuffle=True)\n",
    "data = newsgroup.data\n",
    "target = newsgroup.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809834cf-87d1-48f0-922b-4ae6e0944dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeSpacy(input):\n",
    "    doc = sp(input)\n",
    "    #Tokenizers divide strings into lists of substrings\n",
    "    output = ' '.join([w.lemma_ for w in doc])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c5fcb16-5e32-43cd-ad8d-c00cfa5a9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NVExt(input):\n",
    "    output=\"\"\n",
    "    doc = sp(input)\n",
    "    for w in doc:\n",
    "        if w.pos_ in [\"NOUN\",\"VERB\"]:\n",
    "            output+=w.text+\" \"\n",
    "    output = output[:-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7f46a7-aae6-4856-a945-a20e718815b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, x_test, y_train, y_test, n):\n",
    "    clf = GradientBoostingClassifier(n_estimators=n)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    return f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51c6697-d58e-4a16-87ed-c9685abf8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textProcessing(unprocessedTexts, function=None):\n",
    "    if function is None:\n",
    "        return unprocessedTexts\n",
    "    return [function(text) for text in unprocessedTexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c76c1bd-4e6a-41b5-8da3-68f3b7244ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataExt = textProcessing(data,NVExt)\n",
    "dataProc = textProcessing(dataExt,lemmatizeSpacy)\n",
    "tokenized_documents = [simple_preprocess(text) for text in dataProc]\n",
    "dictionary = corpora.Dictionary(tokenized_documents)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f5987-d068-427d-8170-303489ff4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaArr = ['auto',0.00001,0.0001,0.001,0.01,0.1,0.2,0.4,0.5,0.7,0.9,1,1.2,1.4,1.6,1.8,2,4,6,8,10,14,16,18,20,25,30]\n",
    "betaArr = ['auto',0.00001,0.0001,0.001,0.01,0.1,0.2,0.4,0.5,0.7,0.9,1,1.2,1.4,1.6,1.8,2,4,6,8,10,14,16,18,20,25,30]\n",
    "numArr = [3,5,7,10,15,20,25,30,35,40,50,60,70,80,90,100,120,140,160,180,200,225,250,275,300,350,400]\n",
    "mArr = [10,20,30,40,50,60,70,80,90,100,120,140,160,180,200,225,250,275,300,350,400,450,500]\n",
    "list = []\n",
    "for alp in alphaArr:\n",
    "    for beta in betaArr:\n",
    "        for num in numArr:\n",
    "            for m in mArr:\n",
    "                length = num\n",
    "                lda = LdaModel(bow_corpus, num_topics=num, id2word=dictionary, alpha=alp, eta=beta)\n",
    "                dataVecPre = lda[bow_corpus]\n",
    "                len_data = len(dataVecPre)\n",
    "                dataVec = [0 for o in range(len_data)]\n",
    "                for o in range(len_data):\n",
    "                    curr = []\n",
    "                    a = dataVecPre[o]\n",
    "                    lenCurr = len(a)\n",
    "                    if length!=lenCurr:\n",
    "                        curr = [0 for u in range(length)]\n",
    "                        c = 0\n",
    "                        for u in range(length):\n",
    "                            if c>=lenCurr:\n",
    "                                curr[u] = 0\n",
    "                            else:\n",
    "                                if a[c][0]!=u:\n",
    "                                    curr[u] = 0\n",
    "                                else:\n",
    "                                    curr[u] = a[c][1]\n",
    "                                    c+=1\n",
    "                        dataVec[o] = curr\n",
    "                    else:\n",
    "                        curr = [element[1] for element in a]\n",
    "                        dataVec[o] = curr\n",
    "                dataVec = np.array(dataVec)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(dataVec, target, test_size=0.33)\n",
    "                f1 = train(X_train, X_test, y_train, y_test, m)\n",
    "                list.append([f1,alp,beta,num,m])\n",
    "maxList = [w[0] for w in list]\n",
    "maximum = max(maxList)\n",
    "print(maximum)\n",
    "print(list[maxList.index(maximum)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db3acc-4d74-4c93-a65b-6fc67fd5931b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
