{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2a0b332-c092-45e2-ac49-36e89c38fdb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1976\n",
      "974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from tabulate import tabulate\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "categories = ['rec.autos', 'comp.graphics', 'sci.space']\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories, shuffle=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.33, random_state=42)\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3386abd2-ced3-4a4e-9ab9-4b0963f9b5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: teckjoo@iti.gov.sg (Chua Teck Joo)\\nSubject: Visuallib (3D graphics for Windows)\\nOrganization: Information Technology Institute, National Computer Board, Singapore.\\nLines: 17\\n\\n\\nI am currently looking for a 3D graphics library that runs on MS\\nWindows 3.1.  Are there any such libraries out there other than\\nVisuallib?  (It must run on VGA and should not require any other\\nadd-on graphics cards).\\n\\nFor Visuallib, will it run with Metaware High C compiler v3.0?  Any\\nemail contact for the author of Visuallib?\\n\\nAny help would be much appreciated.  Thanks.\\n\\n\\n-- \\n* Chua, Teck Joo\\t    | Information Technology Institute *\\n* Email: teckjoo@iti.gov.sg | 71 Science Park Drive\\t       *\\n* Phone: (65) 772-0237 \\t    | Singapore (0511)\\t\\t       *\\n* Fax:   (65) 779-1827      |\\t\\t\\t   \\t       *\\n',\n",
       " 'From: graeme@labtam.labtam.oz.au (Graeme Gill)\\nSubject: Re: looking for circle algorithm faster than Bresenhams\\nOrganization: Labtam Australia Pty. Ltd., Melbourne, Australia\\nLines: 28\\n\\nIn article <1993Apr13.025240.8884@nwnexus.WA.COM>, mpdillon@halcyon.com (Michael Dillon) writes:\\n> I have an algorithm similar to Bresenhams line drawing algorithm, that\\n> draws a line by stepping along the minor axis and drawing slices like\\n> AAAA, BBBB, CCCC in the following diagram.\\n> \\n>      AAAA\\n>          BBBB\\n>              CCCC\\n> \\n\\n\\tYes, that\\'s known as \"Bresenhams Run Length Slice Algorithm for\\nIncremental lines\". See Fundamental Algorithms for Computer Graphics,\\nSpringer-Verlag, Berlin Heidelberg 1985.\\n\\n> I have tried to extrapolate this to circles but I can\\'t figure out\\n> how to determine the length of the slices. Any ideas?\\n\\n\\tHmm. I don\\'t think I can help you with this, but you might\\ntake a look at the following:\\n\\n\\t\"Double-Step Incremental Generation of Lines and Circles\",\\nX. Wu and J. G. Rokne, Computer Graphics and Image processing,\\nVol 37, No. 4, Mar. 1987, pp. 331-334\\n\\n\\t\"Double-Step Generation of Ellipses\", X. Wu and J. G. Rokne,\\nIEEE Computer Graphics & Applications, May 1989, pp. 56-69\\n\\n\\tGraeme Gill.\\n',\n",
       " 'From: boyle@bbsls23.bnr (Ian Boyle)\\nSubject: Re: What is \" Volvo \" ?\\nOrganization: BNR Europe Ltd.\\nLines: 20\\nDistribution: world\\nReply-To: boyle@bbsls23.bnr\\nNNTP-Posting-Host: bbsls23.bnr.co.uk\\n\\n> And all of these cars are driven fairly hard. None of them are at the head of\\n> a line of cars going 30 MPH....the first two spend a lot of their operating\\n> life with the speedometer pegged...and the only reason the 84 doesn\\'t is it has\\n> a 120 MPH speedo...\\n> What I want to know is....have all you people who hate Volvos been traumatized\\n> by someone in a 745 Turbo wagon blowing you away on the road, or what?\\n\\n740 Turbo in UK was good for 124mph. Useful for blowing away VW Beetles, though I\\nbelieve the Beetle corners better. \\n\\nI can say without any doubt that I have never been blown away by any Volvo, ever.\\nI\\'ve been blocked into a few car parks though by shit-head Volvo owners who \\'only thought they\\'d be a few minutes\\'. This does not happen with the owners of any other makes of car.\\n\\nNot sure how long the small shit-box Volvos last - too damn long. The worst car I ever drove was a hired 340. In power, handling and ride it was reminiscent of something\\nfrom the 50s, without the character. The 340 only ceased production a couple of years back. I\\'ve only been a passenger in the big Volvos, but that was enough. I ought to go\\nfor a test drive because they offer some neat gifts.\\n\\n\\n\\n\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "664d6268-f39f-470c-8309-532c2e5ecc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "808e2544-5ffa-4fed-aacd-b90b13ef7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeText(input):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #Tokenizers divide strings into lists of substrings\n",
    "    wordList = word_tokenize(input)\n",
    "    output = ' '.join([lemmatizer.lemmatize(w) for w in wordList])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94645376-03dc-40dc-985e-f5c158805519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemText(input):\n",
    "    stemmer = PorterStemmer()\n",
    "    wordList = word_tokenize(input)\n",
    "    output = ' '.join([stemmer.stem(w) for w in wordList])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d463eaa7-3ec5-427a-ad79-c333318fb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, X_test, y_train, y_test, n):\n",
    "    X_train = sp.sparse.csr_matrix(X_train)\n",
    "    X_test = sp.sparse.csr_matrix(X_test)\n",
    "    clf = AdaBoostClassifier(n_estimators=n,algorithm='SAMME')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2f1929f-1c48-479c-a4b5-3f1bc81fb252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textProcessing(unprocessedTexts, function=None):\n",
    "    if function is None:\n",
    "        return unprocessedTexts\n",
    "    return [function(text) for text in unprocessedTexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edec5427-0587-4666-a6fd-a3bda5a9f2ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum: 0.8615943956597978\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=50          None                Stemming            Lemmatizer\n",
      "One-hot       0.8379085074701631  0.8615943956597978  0.8408279052228854\n",
      "Bag of words  0.8379085074701631  0.8615943956597978  0.8378213802122051\n",
      "Tf-idf        0.8109297050773052  0.8256955502940396  0.8366747911749344\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.868666673035172\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=75          None                Stemming            Lemmatizer\n",
      "One-hot       0.861218083082698   0.868666673035172   0.8586641057741649\n",
      "Bag of words  0.861218083082698   0.8633782325155555  0.8635444172300089\n",
      "Tf-idf        0.8488033423311779  0.8668303430604763  0.862256049311158\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.8864072219739598\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=100         None                Stemming            Lemmatizer\n",
      "One-hot       0.8725917961700936  0.8789258173534558  0.862686831002984\n",
      "Bag of words  0.8696452790195986  0.8772411062188511  0.8864072219739598\n",
      "Tf-idf        0.8671788581321679  0.8564749700980165  0.870987220994885\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.9077092942189412\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=200         None                Stemming            Lemmatizer\n",
      "One-hot       0.8988154404418341  0.9060360490073358  0.8890115621265378\n",
      "Bag of words  0.8977297238769999  0.9077092942189412  0.8979271448717724\n",
      "Tf-idf        0.8988547150401639  0.9005429162651266  0.8933214229081217\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.9352835329205609\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=500         None                Stemming            Lemmatizer\n",
      "One-hot       0.9241391520371866  0.9220375534837161  0.9191215630777245\n",
      "Bag of words  0.924187599769842   0.9352835329205609  0.922087378661166\n",
      "Tf-idf        0.9233630300260538  0.9250178820253433  0.9199180397584436\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.9373044293916197\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=750         None                Stemming            Lemmatizer\n",
      "One-hot       0.9324311013083793  0.933260155941304   0.9312445377009285\n",
      "Bag of words  0.9282300955231334  0.9373044293916197  0.9270987530943888\n",
      "Tf-idf        0.9274452516146517  0.9291380544676774  0.9251794643955674\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.9383396377760241\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=1000        None                Stemming            Lemmatizer\n",
      "One-hot       0.9313248817946292  0.9352753829717355  0.9353771319833877\n",
      "Bag of words  0.9313701714948042  0.9383396377760241  0.9322619477405606\n",
      "Tf-idf        0.9293894545826759  0.9342944111083736  0.9302254487968292\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.9413971647907091\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=1250        None                Stemming            Lemmatizer\n",
      "One-hot       0.9323198650979495  0.9403698674227857  0.9342777747309392\n",
      "Bag of words  0.9374926631265704  0.9403876318233013  0.9413971647907091\n",
      "Tf-idf        0.933445980165089   0.9372780581469967  0.9292304195933812\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.9424031962435383\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=1500        None                Stemming            Lemmatizer\n",
      "One-hot       0.9385362881315218  0.93933461836016    0.9312195919418174\n",
      "Bag of words  0.9364030212036962  0.9424031962435383  0.9414096906773506\n",
      "Tf-idf        0.9294111410036899  0.9383374037249469  0.9301990967892523\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.9475606946292969\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=1750        None                Stemming            Lemmatizer\n",
      "One-hot       0.9364167746963695  0.9403799797632664  0.931195822829663\n",
      "Bag of words  0.9405052028389036  0.9475606946292969  0.9424330918081966\n",
      "Tf-idf        0.9304012762104606  0.939347127000098   0.9322394486337399\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.9445229892744528\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=2000        None                Stemming            Lemmatizer\n",
      "One-hot       0.9364469910078761  0.9434256253986761  0.9384097495324583\n",
      "Bag of words  0.939455437930817   0.9445229892744528  0.9444924509134178\n",
      "Tf-idf        0.9284741870376005  0.9383766336186298  0.9312323312456685\n",
      "------------  ------------------  ------------------  ------------------\n"
     ]
    }
   ],
   "source": [
    "for n in [50,75,100,200,500,750,1000,1250,1500,1750,2000]:\n",
    "    tableText = [[\"\" for j in range(4)] for i in range(4)]\n",
    "    tableText[0] = [f\"n={n}\", \"None\", \"Stemming\", \"Lemmatizer\"]\n",
    "    tableText[1][0] = \"One-hot\"\n",
    "    tableText[2][0] = \"Bag of words\"\n",
    "    tableText[3][0] = \"Tf-idf\"\n",
    "    for i in range(3):\n",
    "        if i==0:\n",
    "            func = None\n",
    "        if i==1:\n",
    "            func = stemText\n",
    "        if i==2:\n",
    "            func = lemmatizeText\n",
    "        preprocessedTrain = textProcessing(X_train, func)\n",
    "        preprocessedTest = textProcessing(X_test, func)\n",
    "        for j in range(3):\n",
    "            if j==0:\n",
    "                vect = CountVectorizer(binary=True, stop_words=stop_words)\n",
    "            if j==1:\n",
    "                vect = CountVectorizer(binary=False, stop_words=stop_words)\n",
    "            if j==2:\n",
    "                vect = TfidfVectorizer(stop_words=stop_words)\n",
    "            train_vec = vect.fit_transform(preprocessedTrain)\n",
    "            test_vec = vect.transform(preprocessedTest)\n",
    "            \n",
    "            f1 = train(train_vec, test_vec, y_train, y_test, n)\n",
    "            tableText[j+1][i+1] = f1\n",
    "    list = [max(tableText[i][1:]) for i in range(1,4)]\n",
    "    maximum = max(list)\n",
    "    print(f\"Maximum: {maximum}\")\n",
    "    print(tabulate(tableText))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
