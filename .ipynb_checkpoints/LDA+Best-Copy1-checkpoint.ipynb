{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2322b79-e4db-4a6a-a01f-335734e5ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noun+Verb LemSpacy LDA GBM: 0.8071395816658974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6b59581-e3ed-44e9-8624-e646b745687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LsiModel, LdaModel\n",
    "from gensim import corpora\n",
    "import spacy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')\n",
    "sp = spacy.load('en_core_web_lg')\n",
    "\n",
    "categories = ['rec.autos', 'comp.graphics', 'sci.space']\n",
    "newsgroup = fetch_20newsgroups(categories=categories, shuffle=True)\n",
    "data = newsgroup.data\n",
    "target = newsgroup.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "906b73bf-bdd3-4a54-869f-02a2dc796ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1771\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "809834cf-87d1-48f0-922b-4ae6e0944dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeSpacy(input):\n",
    "    doc = sp(input)\n",
    "    #Tokenizers divide strings into lists of substrings\n",
    "    output = ' '.join([w.lemma_ for w in doc])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c5fcb16-5e32-43cd-ad8d-c00cfa5a9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NVExt(input):\n",
    "    output=\"\"\n",
    "    doc = sp(input)\n",
    "    for w in doc:\n",
    "        if w.pos_ in [\"NOUN\",\"VERB\"]:\n",
    "            output+=w.text+\" \"\n",
    "    output = output[:-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a7f46a7-aae6-4856-a945-a20e718815b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, x_test, y_train, y_test, n):\n",
    "    clf = GradientBoostingClassifier(n_estimators=n)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    return f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a51c6697-d58e-4a16-87ed-c9685abf8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textProcessing(unprocessedTexts, function=None):\n",
    "    if function is None:\n",
    "        return unprocessedTexts\n",
    "    return [function(text) for text in unprocessedTexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c76c1bd-4e6a-41b5-8da3-68f3b7244ea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataExt = textProcessing(data,NVExt)\n",
    "dataProc = textProcessing(dataExt,lemmatizeSpacy)\n",
    "tokenized_documents = [simple_preprocess(text) for text in dataProc]\n",
    "dictionary = corpora.Dictionary(tokenized_documents)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "890f5987-d068-427d-8170-303489ff4d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 auto 3\n",
      "1771\n",
      "[(0, 0.9999997)]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m     curr[u] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m!=\u001b[39mu:\n\u001b[0;32m     30\u001b[0m         curr[u] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "alphaArr = [0.00001]\n",
    "betaArr = ['auto',0.00001]\n",
    "numArr = [3,5]\n",
    "mArr = [10,20]\n",
    "list = []\n",
    "for alp in alphaArr:\n",
    "    for beta in betaArr:\n",
    "        for num in numArr:\n",
    "            for m in mArr:\n",
    "                length = num\n",
    "                print(alp,beta, length)\n",
    "                lda = LdaModel(bow_corpus, num_topics=num, id2word=dictionary, alpha=alp, eta=beta)\n",
    "                dataVecPre = lda[bow_corpus]\n",
    "                len_data = len(dataVecPre)\n",
    "                print(len_data)\n",
    "                dataVec = [0 for o in range(len_data)]\n",
    "                for o in range(len_data):\n",
    "                    curr = []\n",
    "                    a = dataVecPre[o]\n",
    "                    lenCurr = len(a)\n",
    "                    print(a)\n",
    "                    if length!=lenCurr:\n",
    "                        curr = [0 for u in range(length)]\n",
    "                        c = 0\n",
    "                        for u in range(length):\n",
    "                            if c>=lenCurr:\n",
    "                                curr[u] = 0\n",
    "                            else:\n",
    "                                if a[c][0]!=u:\n",
    "                                    curr[u] = 0\n",
    "                                else:\n",
    "                                    curr[u] = a[c][1]\n",
    "                                    c+=1\n",
    "                        dataVec[o] = curr\n",
    "                    else:\n",
    "                        dataVec[o] = a\n",
    "                for el in dataVec:\n",
    "                    if len(el)!=3:\n",
    "                        print(el)\n",
    "                dataVec = np.array(dataVec)\n",
    "                print(len(dataVec))\n",
    "                X_train, X_test, y_train, y_test = train_test_split(dataVec, target, test_size=0.33)\n",
    "                f1 = train(X_train, X_test, y_train, y_test, m)\n",
    "                list.append([f1,apl,beta,num,m])\n",
    "maxList = [w[0] for w in list]\n",
    "maximum = max(maxList)\n",
    "print(maximum)\n",
    "print(list[maxList.index(maximum)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd7e77-1f19-47a3-b3e0-157f11785102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
