{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2a0b332-c092-45e2-ac49-36e89c38fdb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1976\n",
      "974\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from tabulate import tabulate\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "categories = ['rec.autos', 'comp.graphics', 'sci.space']\n",
    "newsgroup = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroup.data, newsgroup.target, test_size=0.33)\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3386abd2-ced3-4a4e-9ab9-4b0963f9b5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: prb@access.digex.com (Pat)\\nSubject: Re: Abyss: breathing fluids\\nArticle-I.D.: access.1psghn$s7r\\nOrganization: Express Access Online Communications USA\\nLines: 19\\nNNTP-Posting-Host: access.digex.net\\n\\nIn article <C4t3K3.498@cck.coventry.ac.uk> enf021@cck.coventry.ac.uk (Achurist) writes:\\n|\\n|I believe the reason is that the lung diaphram gets too tired to pump\\n|the liquid in and out and simply stops breathing after 2-3 minutes.\\n|So if your in the vehicle ready to go they better not put you on \\n|hold, or else!! That's about it. Remember a liquid is several more times\\n|as dense as a gas by its very nature. ~10 I think, depending on the gas\\n|and liquid comparision of course!\\n\\n\\nCould you use some sort of mechanical chest compression as an aid.\\nSorta like the portable Iron Lung?   Put some sort of flex tubing\\naround the 'aquanauts' chest.  Cyclically compress it  and it will\\npush enough on the chest wall to support breathing?????\\n\\nYou'd have to trust your breather,  but in space, you have to trust\\nyour suit anyway.\\n\\npat\\n\",\n",
       " \"From: mmadsen@bonnie.ics.uci.edu (Matt Madsen)\\nSubject: Re: Please Recommend 3D Graphics Library For Mac.\\nNntp-Posting-Host: bonnie.ics.uci.edu\\nReply-To: mmadsen@ics.uci.edu (Matt Madsen)\\nOrganization: Univ. of Calif., Irvine, Info. & Computer Sci. Dept.\\nLines: 27\\n\\nRobert G. Carpenter writes:\\n\\n>Hi Netters,\\n>\\n>I'm building a CAD package and need a 3D graphics library that can handle\\n>some rudimentry tasks, such as hidden line removal, shading, animation, etc.\\n>\\n>Can you please offer some recommendations?\\n>\\n>I'll also need contact info (name, address, email...) if you can find it.\\n>\\n>Thanks\\n>\\n>(Please Post Your Responses, in case others have same need)\\n>\\n>Bob Carpenter\\n>\\n\\nI too would like a 3D graphics library!  How much do C libraries cost\\nanyway?  Can you get the tools used by, say, RenderMan, and can you get\\nthem at a reasonable cost?\\n\\nSorry that I don't have any answers, just questions...\\n\\nMatt Madsen\\nmmadsen@ics.uci.edu\\n\\n\",\n",
       " 'From: tom@igc.apc.org\\nSubject: computer cult\\nNf-ID: #N:cdp:1469100033:000:2451\\nNf-From: cdp.UUCP!tom    Apr 24 09:26:00 1993\\nLines: 59\\n\\n\\nFrom: <tom>\\nSubject: computer cult\\n\\nFrom scott Fri Apr 23 16:31:21 1993\\nReceived: by igc.apc.org (4.1/Revision: 1.77 )\\n\\tid AA16121; Fri, 23 Apr 93 16:31:09 PDT\\nDate: Fri, 23 Apr 93 16:31:09 PDT\\nMessage-Id: <9304232331.AA16121@igc.apc.org>\\nFrom: Scott Weikart <scott>\\nSender: scott\\nTo: cdplist\\nSubject: Next stand-off?\\nStatus: R\\n\\nRedwood City, CA (API) -- A tense stand-off entered its third week\\ntoday as authorities reported no progress in negotiations with\\ncharismatic cult leader Steve Jobs.\\n\\nNegotiators are uncertain of the situation inside the compound, but\\nsome reports suggest that half of the hundreds of followers inside\\nhave been terminated.  Others claim to be staying of their own free\\nwill, but Jobs\\' persuasive manner makes this hard to confirm.\\n\\nIn conversations with authorities, Jobs has given conflicting\\ninformation on how heavily prepared the group is for war with the\\nindustry.  At times, he has claimed to \"have hardware which will blow\\nanything else away\", while more recently he claims they have stopped\\nmanufacturing their own.\\n\\nAgents from the ATF (Apple-Taligent Forces) believe that the group is\\nequipped with serious hardware, including 486-caliber pieces and\\npossibly Canon equipment.\\n\\nThe siege has attracted a variety of spectators, from the curious to\\nother cultists.  Some have offered to intercede in negotiations,\\nincluding a young man who will identify himself only as \"Bill\" and\\nclaims to be the \"MS-iah\".\\n\\nFormer members of the cult, some only recently deprogrammed, speak\\nhesitantly of their former lives, including being forced to work\\n20-hour days, and subsisting on Jolt and Twinkies.  There were\\nfrequent lectures in which they were indoctrinated into a theory of\\n\"interpersonal computing\" which rejects traditional roles.\\n\\nLate-night vigils on Chesapeake Drive are taking their toll on\\nfederal marshals.  Loud rock and roll, mostly Talking Heads, blares\\nthroughout the night.  Some fear that Jobs will fulfill his own\\napocalyptic prophecies, a worry reinforced when the loudspeakers\\ncarry Jobs\\' own speeches -- typically beginning with a chilling \"I\\nwant to welcome you to the \\'Next World\\' \".\\n\\n- - -- \\nRoland J. Schemers III              |            Networking Systems\\nSystems Programmer                  |            G16 Redwood Hall (415) 723-6740\\nDistributed Computing Group         |            Stanford, CA 94305-4122\\nStanford University                 |            schemers@Slapshot.Stanford.EDU\\n\\n\\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "664d6268-f39f-470c-8309-532c2e5ecc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "808e2544-5ffa-4fed-aacd-b90b13ef7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeText(input):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #Tokenizers divide strings into lists of substrings\n",
    "    wordList = word_tokenize(input)\n",
    "    output = ' '.join([lemmatizer.lemmatize(w) for w in wordList])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94645376-03dc-40dc-985e-f5c158805519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemText(input):\n",
    "    stemmer = PorterStemmer()\n",
    "    wordList = word_tokenize(input)\n",
    "    output = ' '.join([stemmer.stem(w) for w in wordList])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d463eaa7-3ec5-427a-ad79-c333318fb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, x_test, y_train, y_test, n):\n",
    "    clf = RandomForestClassifier(random_state=n)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    return f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2f1929f-1c48-479c-a4b5-3f1bc81fb252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textProcessing(unprocessedTexts, function=None):\n",
    "    if function is None:\n",
    "        return unprocessedTexts\n",
    "    return [function(text) for text in unprocessedTexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edec5427-0587-4666-a6fd-a3bda5a9f2ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum: 0.8725532754340956\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=10          None                Stemming            Lemmatizer\n",
      "One-hot       0.8673050484964182  0.8692413652847478  0.8725532754340956\n",
      "Bag of words  0.8639773067191313  0.8608544429819125  0.870445645899432\n",
      "Tf-idf        0.852922453906151   0.867403800357329   0.8561208745715109\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.8744630166517456\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=20          None                Stemming            Lemmatizer\n",
      "One-hot       0.8702537142315718  0.8711550115279761  0.8744630166517456\n",
      "Bag of words  0.8588709074579587  0.8714001243616374  0.8662323492354581\n",
      "Tf-idf        0.8592231081777328  0.8665355632121264  0.8601888340934035\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.8732862066400048\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=30          None                Stemming            Lemmatizer\n",
      "One-hot       0.8591715399883503  0.866290784548742   0.8630540956304472\n",
      "Bag of words  0.8620593554545248  0.8732862066400048  0.8578933962165909\n",
      "Tf-idf        0.8611517480278622  0.855855987941966   0.8612283477332988\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.8712696044787772\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=40          None                Stemming            Lemmatizer\n",
      "One-hot       0.8712696044787772  0.8615023488374179  0.8619935447765639\n",
      "Bag of words  0.8702942369554808  0.8579380638160394  0.8610273372155708\n",
      "Tf-idf        0.8571062494460754  0.8655166739925546  0.8622123626114377\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.8745340952348141\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=50          None                Stemming            Lemmatizer\n",
      "One-hot       0.8690840906084294  0.8745340952348141  0.8587267091936115\n",
      "Bag of words  0.8661082293949061  0.855786293064346   0.8550380115445474\n",
      "Tf-idf        0.8572156036474485  0.849620263396306   0.8561368429241679\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.8723714811157083\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=60          None                Stemming            Lemmatizer\n",
      "One-hot       0.8723714811157083  0.8680460377614635  0.8693740613045332\n",
      "Bag of words  0.8670252158296946  0.8650839884730441  0.8610531194250226\n",
      "Tf-idf        0.8601685879306947  0.8613352176096737  0.8603148223009726\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.8750794791094875\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=70          None                Stemming            Lemmatizer\n",
      "One-hot       0.8643024852000025  0.8684416279585899  0.8633791721571965\n",
      "Bag of words  0.8641690744717984  0.8653162114797264  0.8750794791094875\n",
      "Tf-idf        0.850877991172836   0.86223295322498    0.8634203183808197\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.87767659729585\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=80          None                Stemming            Lemmatizer\n",
      "One-hot       0.8596133732829092  0.8755515200342116  0.8713520947738551\n",
      "Bag of words  0.86383930277165    0.87767659729585    0.871412176187212\n",
      "Tf-idf        0.859859855407163   0.8633719921867055  0.8601012270226812\n",
      "------------  ------------------  ------------------  ------------------\n"
     ]
    }
   ],
   "source": [
    "for n in [10,20,30,40,50,60,70,80]:\n",
    "    tableText = [[\"\" for j in range(4)] for i in range(4)]\n",
    "    tableText[0] = [f\"n={n}\", \"None\", \"Stemming\", \"Lemmatizer\"]\n",
    "    tableText[1][0] = \"One-hot\"\n",
    "    tableText[2][0] = \"Bag of words\"\n",
    "    tableText[3][0] = \"Tf-idf\"\n",
    "    for i in range(3):\n",
    "        if i==0:\n",
    "            func = None\n",
    "        if i==1:\n",
    "            func = stemText\n",
    "        if i==2:\n",
    "            func = lemmatizeText\n",
    "        preprocessedTrain = textProcessing(X_train, func)\n",
    "        preprocessedTest = textProcessing(X_test, func)\n",
    "        for j in range(3):\n",
    "            if j==0:\n",
    "                vect = CountVectorizer(binary=True, stop_words=stop_words)\n",
    "            if j==1:\n",
    "                vect = CountVectorizer(binary=False, stop_words=stop_words)\n",
    "            if j==2:\n",
    "                vect = TfidfVectorizer(stop_words=stop_words)\n",
    "            train_vec = vect.fit_transform(preprocessedTrain)\n",
    "            test_vec = vect.transform(preprocessedTest)\n",
    "\n",
    "            f1 = train(train_vec, test_vec, y_train, y_test, n)\n",
    "            tableText[j+1][i+1] = f1\n",
    "    list = [max(tableText[i][1:]) for i in range(1,4)]\n",
    "    maximum = max(list)\n",
    "    print(f\"Maximum: {maximum}\")\n",
    "    print(tabulate(tableText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23ca2a-e675-461a-81a7-de514c0a4d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
